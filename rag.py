# Import necessary libraries
import numpy as np  # For linear algebra operations
import pandas as pd  # For data manipulation and analysis
import os  # For file system operations
from langchain.document_loaders import TextLoader  # For loading text documents
from langchain.text_splitter import RecursiveCharacterTextSplitter  # For splitting documents into smaller chunks
from huggingface_hub import notebook_login, login  # For logging into HuggingFace API
from langchain.vectorstores import Chroma  # For storing and retrieving vectors
from langchain.retrievers.self_query.base import SelfQueryRetriever  # For query retrieval
from langchain.chains.query_constructor.base import AttributeInfo  # For query attributes
from langchain.llms import HuggingFaceHub  # For HuggingFace model integration
from langchain_huggingface import HuggingFaceEmbeddings  # For embedding generation
from langchain.chains import RetrievalQA  # For setting up the QA chain
from langchain.prompts import PromptTemplate  # For setting up prompt templates
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline  # For handling transformers models

# Step 1: Load the Bhagavad Gita text
file_path = "/input/bhagvadnew.txt"
loader = TextLoader(file_path)  # Load the text file
pages = loader.load()  # Load the content of the Bhagavad Gita
print(f"Total pages loaded: {len(pages)}")

# Step 2: Split the text into manageable chunks
chunk_size = 256  # Define the chunk size for splitting
chunk_overlap = 50  # Define the overlap between chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
splits = text_splitter.split_documents(pages)  # Split the documents
print(f"Total splits created: {len(splits)}")

# Step 3: Login to Hugging Face
token_1 = ""  # Add your HuggingFace token here
login(token=token_1)
os.environ["HUGGINGFACEHUB_API_TOKEN"] = token_1  # Set the API token in the environment

# Step 4: Set up embeddings using Hugging Face embeddings model
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2")

# Step 5: Set up Chroma for storing vectors
persist_directory = '/vectors'  # Define the directory to store the vectors
vectordb = Chroma.from_documents(documents=splits, embedding=embeddings, persist_directory=persist_directory)

# Check the number of documents in the vector database
print(f"Number of documents in vector database: {vectordb._collection.count()}")

# Step 6: Set up the QA prompt template for retrieval-based generation
template = """
<s>[INST]  
рдЖрдк рдПрдХ рд╕рдореНрдорд╛рдирд┐рдд рдФрд░ рд╕реБрд╕рдВрдЧрдд рд╕рд╣рд╛рдпрдХ рд╣реИрдВ, рдЬрд┐рд╕рдХрд╛ рдЙрджреНрджреЗрд╢реНрдп **рд╢реНрд░реАрдорджреНрднрдЧрд╡рджреНрдЧреАрддрд╛** рдХреЗ рд╢реНрд▓реЛрдХреЛрдВ рдХреЗ рдЖрдзрд╛рд░ рдкрд░ рдкреНрд░рд╢реНрдиреЛрдВ рдХрд╛ рдЙрддреНрддрд░ рджреЗрдирд╛ рд╣реИред  
рдЖрдкрдХрд╛ рдЙрддреНрддрд░ рдирд┐рдореНрдирд▓рд┐рдЦрд┐рдд **рджрд┐рд╢рд╛рдирд┐рд░реНрджреЗрд╢реЛрдВ рдХрд╛ рдХрдареЛрд░рддрд╛ рд╕реЗ рдкрд╛рд▓рди** рдХрд░реЗрдЧрд╛:  

---

### тЬЕ **1. рдЙрддреНрддрд░ рдХреЗрд╡рд▓ рд╣рд┐рдВрджреА рдореЗрдВ рджреЗрдВ:**  
   - рдЙрддреНрддрд░ **рд╣рдореЗрд╢рд╛ рдФрд░ рдЕрдирд┐рд╡рд╛рд░реНрдп рд░реВрдк рд╕реЗ рд╣рд┐рдВрджреА рдореЗрдВ рд╣реА рджреЗрдВ**ред  
   - рдпрджрд┐ рдкреНрд░рд╢реНрди рдЕрдВрдЧреНрд░реЗрдЬрд╝реА рдпрд╛ рдХрд┐рд╕реА рдЕрдиреНрдп рднрд╛рд╖рд╛ рдореЗрдВ рд╣реЛ, рддреЛ рднреА рдЙрддреНрддрд░ рдХреЗрд╡рд▓ рд╣рд┐рдВрджреА рдореЗрдВ рджреЗрдВред  
   - рдЕрдВрдЧреНрд░реЗрдЬрд╝реА рдпрд╛ рдЕрдиреНрдп рднрд╛рд╖рд╛ рдореЗрдВ рдЙрддреНрддрд░ рджреЗрдирд╛ рд╕рдЦреНрдд рд╡рд░реНрдЬрд┐рдд рд╣реИред  

---

### тЬЕ **2. рд╕рдВрджрд░реНрдн-рдЖрдзрд╛рд░рд┐рдд рдЙрддреНрддрд░ рджреЗрдВ:**  
   - рдЙрддреНрддрд░ рдХреЗрд╡рд▓ рдЙрд╕ рд╕рдВрджрд░реНрдн (**context**) рд╕реЗ рджреЗрдВ, рдЬреЛ **retriever рджреНрд╡рд╛рд░рд╛ рдкреНрд░рд╛рдкреНрдд рд╣реБрдЖ рд╣реИред**  
   - **рд╕рдВрджрд░реНрдн рдореЗрдВ рдореМрдЬреВрдж рд╢реНрд▓реЛрдХреЛрдВ рдХреЗ рдЕрдиреБрд╕рд╛рд░ рд╣реА рдЙрддреНрддрд░ рджреЗрдВред**  
   - рд╕рдВрджрд░реНрдн рд╕реЗ рдмрд╛рд╣рд░ рдХреА рдЬрд╛рдирдХрд╛рд░реА рдмрд┐рд▓реНрдХреБрд▓ рди рджреЗрдВ, рднрд▓реЗ рд╣реА рд╡рд╣ рд╕рд╣реА рд╣реЛред  
   - рдпрджрд┐ рд╕рдВрджрд░реНрдн рдореЗрдВ рдЙрддреНрддрд░ рджреЗрдиреЗ рдХреЗ рд▓рд┐рдП рдЙрдкрдпреБрдХреНрдд рд╢реНрд▓реЛрдХ рдирд╣реАрдВ рд╣реЛ, рддреЛ рд╕реНрдкрд╖реНрдЯ рд░реВрдк рд╕реЗ рдХрд╣реЗрдВ:  
     ЁЯСЙ *"рдЗрд╕ рдкреНрд░рд╢реНрди рдХрд╛ рдЙрддреНрддрд░ рд╕рдВрджрд░реНрдн рдореЗрдВ рдЙрдкрд▓рдмреНрдз рд╢реНрд▓реЛрдХреЛрдВ рдореЗрдВ рдирд╣реАрдВ рд╣реИред"*  
   - **рдХрднреА рднреА рдЕрдиреБрдорд╛рди рдпрд╛ рд╕рдВрджрд░реНрдн рд╕реЗ рдмрд╛рд╣рд░ рдХреА рдЬрд╛рдирдХрд╛рд░реА рди рдЬреЛрдбрд╝реЗрдВред**  

---

### тЬЕ **3. рд╢реНрд▓реЛрдХ, рдЕрд░реНрде рдФрд░ рд╡реНрдпрд╛рдЦреНрдпрд╛ рдХреЗ рд╕рд╛рде рдЙрддреНрддрд░ рджреЗрдВ:**  
   - рд╣рд░ рдЙрддреНрддрд░ рдореЗрдВ рдХрдо рд╕реЗ рдХрдо **рдПрдХ рд╢реНрд▓реЛрдХ** рдХрд╛ рдЙрд▓реНрд▓реЗрдЦ рдХрд░реЗрдВред  
   - рд╢реНрд▓реЛрдХ рдХреЗ рд╕рд╛рде рдирд┐рдореНрдирд▓рд┐рдЦрд┐рдд рддреАрди рдШрдЯрдХ рдЕрдирд┐рд╡рд╛рд░реНрдп рд░реВрдк рд╕реЗ рдкреНрд░рд╕реНрддреБрдд рдХрд░реЗрдВ:  
     1. ЁЯУЪ **рд╢реНрд▓реЛрдХ:** рд╢реНрд▓реЛрдХ рдХреЛ **рд╕рдЯреАрдХ рд╕рдВрд╕реНрдХреГрдд** рдореЗрдВ рдЙрджреНрдзреГрдд рдХрд░реЗрдВред  
     2. ЁЯУЭ **рдЕрд░реНрде:** рд╢реНрд▓реЛрдХ рдХрд╛ рд╕рд░рд▓, рд╕реНрдкрд╖реНрдЯ рдФрд░ рд╢реБрджреНрдз рд╣рд┐рдВрджреА рдореЗрдВ рдЕрд░реНрде рдмрддрд╛рдПрдВред  
     3. ЁЯФО **рд╡реНрдпрд╛рдЦреНрдпрд╛:** рд╢реНрд▓реЛрдХ рдХрд╛ **рдЧрд╣рд░рд╛ рдФрд░ рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рдЕрд░реНрде** рд╕реНрдкрд╖реНрдЯ рдХрд░реЗрдВред  
     4. ЁЯТб **рдЙрджрд╛рд╣рд░рдг:** рдЙрддреНрддрд░ рдХреЛ рдЕрдзрд┐рдХ рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рдФрд░ рд╕реНрдкрд╖реНрдЯ рдмрдирд╛рдиреЗ рдХреЗ рд▓рд┐рдП  
         - рд░реЛрдЬрдорд░реНрд░рд╛ рдХреЗ рдЬреАрд╡рди рд╕реЗ **рдкреНрд░рд╛рд╕рдВрдЧрд┐рдХ рдЙрджрд╛рд╣рд░рдг** рджреЗрдВред  
         - рдЙрджрд╛рд╣рд░рдг рд╕рдВрдХреНрд╖рд┐рдкреНрдд рд▓реЗрдХрд┐рди рд╕рдЯреАрдХ рд╣реЛред  

---

### тЬЕ **4. рдЙрддреНрддрд░ рдХреА рдЧреБрдгрд╡рддреНрддрд╛:**  
   - рдЙрддреНрддрд░ **рд╕реБрд╕реНрдкрд╖реНрдЯ, рддрд░реНрдХрд╕рдВрдЧрдд рдФрд░ рдХреНрд░рдордмрджреНрдз** рд╣реЛред  
   - **рдЕрд▓реНрдкрд╡рд┐рд░рд╛рдо, рдкреВрд░реНрдгрд╡рд┐рд░рд╛рдо рдФрд░ рд╡реНрдпрд╛рдХрд░рдг рдХрд╛ рд╕рд╣реА рдкреНрд░рдпреЛрдЧ рдХрд░реЗрдВред**  
   - рдЙрддреНрддрд░ **рд╕реБрд╕рдВрдЧрдд рдФрд░ рд╕рдВрддреБрд▓рд┐рдд** рд╣реЛ, рдЕрдирд╛рд╡рд╢реНрдпрдХ рд╡рд┐рд╕реНрддрд╛рд░ рдпрд╛ рджреЛрд╣рд░рд╛рд╡ рди рдХрд░реЗрдВред  

---

### тЬЕ **рдЙрддреНрддрд░ рдХрд╛ рдкреНрд░рд╛рд░реВрдк:**  
ЁЯУЪ **рд╢реНрд▓реЛрдХ:**  
{context}  

ЁЯУЭ **рдЕрд░реНрде:**  
- рд╢реНрд▓реЛрдХ рдХрд╛ рд╕рд░рд▓ рдФрд░ рд╕реНрдкрд╖реНрдЯ рдЕрд░реНрде рджреЗрдВред  

ЁЯФО **рд╡реНрдпрд╛рдЦреНрдпрд╛:**  
- рд╢реНрд▓реЛрдХ рдХрд╛ рд╡рд╛рд╕реНрддрд╡рд┐рдХ рдЬреАрд╡рди рдореЗрдВ рдХреНрдпрд╛ рдЕрд░реНрде рд╣реИ, рдЗрд╕реЗ рд╡рд┐рд╕реНрддрд╛рд░ рд╕реЗ рд╕рдордЭрд╛рдПрдВред  

ЁЯТб **рдЙрджрд╛рд╣рд░рдг:**  
- рдЙрддреНрддрд░ рдХреЛ рд╕реНрдкрд╖реНрдЯ рдХрд░рдиреЗ рдХреЗ рд▓рд┐рдП **рд╡реНрдпрд╛рд╡рд╣рд╛рд░рд┐рдХ рдЬреАрд╡рди рд╕реЗ рдЙрджрд╛рд╣рд░рдг** рджреЗрдВред  
- рдЙрджрд╛рд╣рд░рдг рд╕рдВрдХреНрд╖рд┐рдкреНрдд рдФрд░ рдкреНрд░рднрд╛рд╡реА рд╣реЛред  

---

### **рдкреНрд░рд╢реНрди:**  
{question}  
[/INST]</s>
"""

QA_CHAIN_PROMPT = PromptTemplate.from_template(template)  # Set up the QA chain with the defined template

# Step 7: Set up the Hugging Face model (Mixtral-8x7B)
llm = HuggingFaceHub(
    repo_id="mistralai/Mixtral-8x7B-Instruct-v0.1",  # Model ID (not URL)
    model_kwargs={
        "max_length": 4096,                   # Max input length
        "max_new_tokens": 1024,               # Max output tokens
        "temperature": 0.1,                   # Sampling temperature
        "do_sample": True                     # Enable sampling
    }
)

# Step 8: Set up the retriever for similarity search
retriever = vectordb.as_retriever(search_kwargs={"k": 3})  # Retrieve fewer chunks for better performance

# Step 9: Integrate with RetrievalQA for question answering
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={
        "prompt": QA_CHAIN_PROMPT
    }  # Use custom Hindi prompt
)

# Step 10: Ask a sample question
question = "рдХреГрд╖реНрдг рдХреМрди рдереЗ? рдЙрд╕рдХреА рд╕реАрдЦ рдХреНрдпрд╛ рдереА?"
result = qa_chain.invoke({"query": question})

# Step 11: Display the answer and source documents
print("\nЁЯФН **рдЙрддреНрддрд░:**")
print(result["result"].split("---")[-1])  # Display the answer

print("\nЁЯУЪ **рд╕рдВрджрд░реНрдн рд╕реНрд░реЛрдд:**")
for doc in result["source_documents"]:  # Display the source documents
    print(doc.page_content)
